{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction using HoG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "import glob\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder,MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "input0 = './train/Preprocessed/'\n",
    "temp = ['Bowlegs', 'Knock Knees', 'Normal Knee']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in temp:\n",
    "    for filename in os.listdir('./train/Preprocessed/' + j):\n",
    "        img = cv2.resize(cv2.imread('./train/Preprocessed/' + j + '/' + filename, 0), (256, 256))\n",
    "        #initialise HoG descriptor\n",
    "        cell_size = (32, 32)  # h x w in pixels\n",
    "        block_size = (2, 2)  # h x w in cells\n",
    "        nbins = 9  # number of orientation bins\n",
    "\n",
    "\n",
    "        # winSize is the size of the image cropped to an multiple of the cell size\n",
    "        # cell_size is the size of the cells of the img patch over which to calculate the histograms\n",
    "        # block_size is the number of cells which fit in the patch\n",
    "\n",
    "        hog = cv2.HOGDescriptor(_winSize=(img.shape[1] // cell_size[1] * cell_size[1],\n",
    "                                        img.shape[0] // cell_size[0] * cell_size[0]),\n",
    "                    _blockSize=(block_size[1] * cell_size[1],\n",
    "                                block_size[0] * cell_size[0]),\n",
    "                    _blockStride=(cell_size[1], cell_size[0]),\n",
    "                    _cellSize=(cell_size[1], cell_size[0]),\n",
    "                    _nbins=nbins)\n",
    "\n",
    "\n",
    "        descriptor = hog.compute(img)\n",
    "        out=pd.DataFrame(descriptor)\n",
    "\n",
    "        #append to the csv file\n",
    "        out = out.transpose()\n",
    "        csv_data=out.to_csv('./HoG/HoG_' + j + '.csv', mode='a', header=False, index=False)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv('./HoG/HoG_Bowlegs.csv')\n",
    "data2 = pd.read_csv('./HoG/HoG_Knock Knees.csv')\n",
    "data3 = pd.read_csv('./HoG/HoG_Normal Knee.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=5)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#performing kmeans on each class\n",
    "#Bowlegs\n",
    "kmeans1 = KMeans(n_clusters=5)\n",
    "kmeans1.fit(data1)\n",
    "\n",
    "#Knock Knees\n",
    "kmeans2 = KMeans(n_clusters=5)\n",
    "kmeans2.fit(data2)\n",
    "\n",
    "#Normal\n",
    "kmeans3 = KMeans(n_clusters=5)\n",
    "kmeans3.fit(data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "histogram of bowed\n",
      "(array([28, 14, 12, 35, 21], dtype=int64), array([0, 1, 2, 3, 4, 5])) \n",
      "\n",
      "histogram of knocked\n",
      "(array([56, 50, 33, 57, 52], dtype=int64), array([0, 1, 2, 3, 4, 5])) \n",
      "\n",
      "histogram of normal\n",
      "(array([56, 45, 72, 43, 32], dtype=int64), array([0, 1, 2, 3, 4, 5])) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "hist1=np.histogram(kmeans1.labels_,bins=[0,1,2,3,4,5])\n",
    "hist2=np.histogram(kmeans2.labels_,bins=[0,1,2,3,4,5])\n",
    "hist3=np.histogram(kmeans3.labels_,bins=[0,1,2,3,4,5])\n",
    "\n",
    "print('histogram of bowed')\n",
    "print(hist1,\"\\n\")\n",
    "\n",
    "print('histogram of knocked')\n",
    "print(hist2,\"\\n\")\n",
    "\n",
    "print('histogram of normal')\n",
    "print(hist3,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performing kmeans prediction of the entire apple dataset with the pretrained kmeans model\n",
    "#initialising i=0; as its the first class\n",
    "i=0\n",
    "input0 = 'train/Preprocessed/'\n",
    "for j in temp:\n",
    "    data=[]\n",
    "    for filename in os.listdir(input0 + j):\n",
    "        path = input0 + j + '/' + filename\n",
    "        a=cv2.imread(path)\n",
    "        resize=(256,256)\n",
    "        img=cv2.resize(a,resize)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        #initialise sift descriptor\n",
    "        cell_size = (32, 32)  # h x w in pixels\n",
    "        block_size = (2, 2)  # h x w in cells\n",
    "        nbins = 9  # number of orientation bins\n",
    "\n",
    "\n",
    "        # winSize is the size of the image cropped to an multiple of the cell size\n",
    "        # cell_size is the size of the cells of the img patch over which to calculate the histograms\n",
    "        # block_size is the number of cells which fit in the patch\n",
    "\n",
    "        hog = cv2.HOGDescriptor(_winSize=(img.shape[1] // cell_size[1] * cell_size[1],\n",
    "                                        img.shape[0] // cell_size[0] * cell_size[0]),\n",
    "                    _blockSize=(block_size[1] * cell_size[1],\n",
    "                                block_size[0] * cell_size[0]),\n",
    "                    _blockStride=(cell_size[1], cell_size[0]),\n",
    "                    _cellSize=(cell_size[1], cell_size[0]),\n",
    "                    _nbins=nbins)\n",
    "\n",
    "\n",
    "        descriptor = hog.compute(img)\n",
    "        out=pd.DataFrame(descriptor)\n",
    "\n",
    "        #drop first coloumn as it's the no of feature detected. Not required.\n",
    "        #append to the csv file\n",
    "        out = out.transpose()\n",
    "        \n",
    "        #predict values of feature vector with pretrained kmeans\n",
    "        #ValueError: Buffer dtype mismatch, expected 'float' but got 'double', in order to avoid this dtype=np.double\n",
    "        array_double = np.array(out, dtype=np.double)\n",
    "\n",
    "        a=kmeans1.predict(array_double)\n",
    "        hist=np.histogram(a,bins=[0,1,2,3,4,5])\n",
    "        #append the dataframe into the array in append mode, the array will only have 5 values which will store the values in a row\n",
    "        data.append(hist[0])\n",
    "    \n",
    "    #convert Array to Dataframe and append to the list\n",
    "    Output = pd.DataFrame(data)\n",
    "    #add row class \n",
    "    Output[\"Class\"] = i \n",
    "    csv_data=Output.to_csv('./HoG/HoG_'+ j + 'Final.csv', mode='a', index=False)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0  1  2  3  4  Class\n",
      "0    0  0  1  0  0      0\n",
      "1    0  0  0  1  0      0\n",
      "2    0  0  0  1  0      0\n",
      "3    0  0  0  1  0      0\n",
      "4    0  0  0  1  0      0\n",
      "..  .. .. .. .. ..    ...\n",
      "244  0  0  0  0  1      2\n",
      "245  1  0  0  0  0      2\n",
      "246  1  0  0  0  0      2\n",
      "247  0  0  0  0  1      2\n",
      "248  1  0  0  0  0      2\n",
      "\n",
      "[609 rows x 6 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sumit\\AppData\\Local\\Temp\\ipykernel_4088\\1115290331.py:2: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  temp1 = final.append(pd.read_csv('./HoG/HoG_Knock KneesFinal.csv'))\n",
      "C:\\Users\\sumit\\AppData\\Local\\Temp\\ipykernel_4088\\1115290331.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  tcc = temp1.append(tc)\n"
     ]
    }
   ],
   "source": [
    "final = pd.read_csv('./HoG/HoG_BowlegsFinal.csv')\n",
    "temp1 = final.append(pd.read_csv('./HoG/HoG_Knock KneesFinal.csv'))\n",
    "tc = pd.read_csv('./HoG/HoG_Normal KneeFinal.csv')\n",
    "tcc = temp1.append(tc)\n",
    "\n",
    "csv_data = tcc.to_csv('./HoG/HoG_FeatureFinal.csv', mode='a', index=False)\n",
    "print(tcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./HoG/HoG_FeatureFinal.csv')\n",
    "X_train = df.iloc[:, 0:5]\n",
    "Y_train = df.iloc[:, 5:6]\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(X_train, Y_train, \n",
    "                                                      test_size=0.5, \n",
    "                                                      stratify=Y_train, \n",
    "                                                      random_state=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sumit\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=7)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "  \n",
    "knn.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results obtained for KNN\n",
      "\n",
      "Results obtained on Training Data\n",
      "Accuracy on Train data :  0.48355263157894735\n",
      "Precision:  0.3271697207920427\n",
      "Recall:  0.3951612903225807\n",
      "F-score:  0.33698366954851106\n",
      "\n",
      "Results obtained on Testing Data\n",
      "Accuracy on Test data :  0.46557377049180326\n",
      "Precision:  0.31510975563839\n",
      "Recall:  0.37866666666666665\n",
      "F-score:  0.32176444245409763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sumit\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\sumit\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print('\\nResults obtained for KNN')\n",
    "\n",
    "# accuracy on training data\n",
    "y_pred_t = knn.predict(train_x)\n",
    "train_data_accuracy = accuracy_score(y_pred_t, train_y)\n",
    "\n",
    "print('\\nResults obtained on Training Data')\n",
    "print('Accuracy on Train data : ', train_data_accuracy)\n",
    "print(\"Precision: \", precision_score(train_y, y_pred_t, average='macro'))\n",
    "print(\"Recall: \", recall_score(train_y, y_pred_t, average='macro'))\n",
    "print(\"F-score: \", f1_score(train_y, y_pred_t, average='macro'))\n",
    "\n",
    "# Accuracy on test data\n",
    "y_pred_ts = knn.predict(valid_x)\n",
    "test_data_accuracy = accuracy_score(y_pred_ts, valid_y)\n",
    "\n",
    "print('\\nResults obtained on Testing Data')\n",
    "print('Accuracy on Test data : ', test_data_accuracy)\n",
    "print(\"Precision: \", precision_score(valid_y, y_pred_ts, average='macro'))\n",
    "print(\"Recall: \", recall_score(valid_y, y_pred_ts, average='macro'))\n",
    "print(\"F-score: \", f1_score(valid_y, y_pred_ts, average='macro'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XgBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:18:44] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sumit\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\sumit\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:98: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\sumit\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\sumit\\anaconda3\\lib\\site-packages\\xgboost\\data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "              gamma=0, gpu_id=-1, importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.300000012,\n",
       "              max_delta_step=0, max_depth=3, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
       "              num_parallel_tree=1, objective='multi:softprob', predictor='auto',\n",
       "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=None,\n",
       "              subsample=1, tree_method='exact', validate_parameters=1,\n",
       "              verbosity=None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "classifier = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    reg_lambda=1,\n",
    "    gamma=0,\n",
    "    max_depth=3\n",
    ")\n",
    "\n",
    "classifier.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results obtained for XgBoost\n",
      "\n",
      "Results obtained on Training Data\n",
      "Accuracy on Train data :  0.5526315789473685\n",
      "Precision:  0.5144132016369863\n",
      "Recall:  0.4777265745007681\n",
      "F-score:  0.4705938778389054\n",
      "\n",
      "Results obtained on Testing Data\n",
      "Accuracy on Test data :  0.5672131147540984\n",
      "Precision:  0.49294214380609835\n",
      "Recall:  0.4783030303030303\n",
      "F-score:  0.46171155205851183\n"
     ]
    }
   ],
   "source": [
    "print('\\nResults obtained for XgBoost')\n",
    "\n",
    "# accuracy on training data\n",
    "y_pred_t = classifier.predict(train_x)\n",
    "train_data_accuracy = accuracy_score(y_pred_t, train_y)\n",
    "\n",
    "print('\\nResults obtained on Training Data')\n",
    "print('Accuracy on Train data : ', train_data_accuracy)\n",
    "print(\"Precision: \", precision_score(train_y, y_pred_t, average='macro'))\n",
    "print(\"Recall: \", recall_score(train_y, y_pred_t, average='macro'))\n",
    "print(\"F-score: \", f1_score(train_y, y_pred_t, average='macro'))\n",
    "\n",
    "# Accuracy on test data\n",
    "y_pred_ts = classifier.predict(valid_x)\n",
    "test_data_accuracy = accuracy_score(y_pred_ts, valid_y)\n",
    "\n",
    "print('\\nResults obtained on Testing Data')\n",
    "print('Accuracy on Test data : ', test_data_accuracy)\n",
    "print(\"Precision: \", precision_score(valid_y, y_pred_ts, average='macro'))\n",
    "print(\"Recall: \", recall_score(valid_y, y_pred_ts, average='macro'))\n",
    "print(\"F-score: \", f1_score(valid_y, y_pred_ts, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sumit\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneVsOneClassifier(estimator=SVC())"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "\n",
    "subclassifier = SVC(kernel='rbf')\n",
    "classifier = OneVsOneClassifier(estimator=subclassifier)\n",
    "classifier.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results obtained for SVM\n",
      "\n",
      "Results obtained on Training Data\n",
      "Accuracy on Train data :  0.5526315789473685\n",
      "Precision:  0.5144132016369863\n",
      "Recall:  0.4777265745007681\n",
      "F-score:  0.4705938778389054\n",
      "\n",
      "Results obtained on Testing Data\n",
      "Accuracy on Test data :  0.5672131147540984\n",
      "Precision:  0.49294214380609835\n",
      "Recall:  0.4783030303030303\n",
      "F-score:  0.46171155205851183\n"
     ]
    }
   ],
   "source": [
    "print('\\nResults obtained for SVM')\n",
    "\n",
    "# accuracy on training data\n",
    "y_pred_t = classifier.predict(train_x)\n",
    "train_data_accuracy = accuracy_score(y_pred_t, train_y)\n",
    "\n",
    "print('\\nResults obtained on Training Data')\n",
    "print('Accuracy on Train data : ', train_data_accuracy)\n",
    "print(\"Precision: \", precision_score(train_y, y_pred_t, average='macro'))\n",
    "print(\"Recall: \", recall_score(train_y, y_pred_t, average='macro'))\n",
    "print(\"F-score: \", f1_score(train_y, y_pred_t, average='macro'))\n",
    "\n",
    "# Accuracy on test data\n",
    "y_pred_ts = classifier.predict(valid_x)\n",
    "test_data_accuracy = accuracy_score(y_pred_ts, valid_y)\n",
    "\n",
    "print('\\nResults obtained on Testing Data')\n",
    "print('Accuracy on Test data : ', test_data_accuracy)\n",
    "print(\"Precision: \", precision_score(valid_y, y_pred_ts, average='macro'))\n",
    "print(\"Recall: \", recall_score(valid_y, y_pred_ts, average='macro'))\n",
    "print(\"F-score: \", f1_score(valid_y, y_pred_ts, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sumit\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "classifier = BernoulliNB()\n",
    "classifier.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results obtained for Naive Bayes\n",
      "\n",
      "Results obtained on Training Data\n",
      "Accuracy on Train data :  0.5526315789473685\n",
      "Precision:  0.5144132016369863\n",
      "Recall:  0.4777265745007681\n",
      "F-score:  0.4705938778389054\n",
      "\n",
      "Results obtained on Testing Data\n",
      "Accuracy on Test data :  0.5672131147540984\n",
      "Precision:  0.49294214380609835\n",
      "Recall:  0.4783030303030303\n",
      "F-score:  0.46171155205851183\n"
     ]
    }
   ],
   "source": [
    "print('\\nResults obtained for Naive Bayes')\n",
    "\n",
    "# accuracy on training data\n",
    "y_pred_t = classifier.predict(train_x)\n",
    "train_data_accuracy = accuracy_score(y_pred_t, train_y)\n",
    "\n",
    "print('\\nResults obtained on Training Data')\n",
    "print('Accuracy on Train data : ', train_data_accuracy)\n",
    "print(\"Precision: \", precision_score(train_y, y_pred_t, average='macro'))\n",
    "print(\"Recall: \", recall_score(train_y, y_pred_t, average='macro'))\n",
    "print(\"F-score: \", f1_score(train_y, y_pred_t, average='macro'))\n",
    "\n",
    "# Accuracy on test data\n",
    "y_pred_ts = classifier.predict(valid_x)\n",
    "test_data_accuracy = accuracy_score(y_pred_ts, valid_y)\n",
    "\n",
    "print('\\nResults obtained on Testing Data')\n",
    "print('Accuracy on Test data : ', test_data_accuracy)\n",
    "print(\"Precision: \", precision_score(valid_y, y_pred_ts, average='macro'))\n",
    "print(\"Recall: \", recall_score(valid_y, y_pred_ts, average='macro'))\n",
    "print(\"F-score: \", f1_score(valid_y, y_pred_ts, average='macro'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e11265db24d7d7b1d9a614b6c8aed899b36881c20eb26f07e2c77f204b3164ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
